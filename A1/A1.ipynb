{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prove that under *Gaussian Assumption* Linear Regression Amounts To *Least square*:\n",
    "**Let** us assume that the target variables and the inputs are related via the equation \n",
    "\\begin{equation}\n",
    "y^{(i)} =\\theta^{T}x^{(i)}   + \\epsilon^{(i)}\n",
    "%$$y^{(i)} = \\theta^{T}x^{(i)} \\ + \\ \\epsilon^{(i)}$$\n",
    "\\end{equation}\n",
    "where $\\epsilon^{(i)}$ is an error term that captures either unmodeled effects , or random noise. Let us further assume that the $\\epsilon^{(i)}$ are distributed IID according to a Gaussian distribution (also called a Normal distribution)with mean zero  and some variation $\\sigma$ . We can write this assumption as \n",
    "$$ \\epsilon^{(i)} \\sim N(0,\\sigma^2)$$\n",
    "\n",
    "\n",
    "\n",
    "I.e., the density of $\\epsilon^{i}$ is given by \n",
    "$$ p(\\epsilon^{i}) = \\frac{1}{\\sqrt{2\\pi }\\sigma} \\ \\ exp \\ \\bigg(-\\frac{(\\epsilon^{i})^2}{2\\sigma^2}\\bigg) $$  This implies  that $$p(y^i|X^i; \\theta) =  \\frac{1}{\\sqrt{2\\pi }\\sigma} \\ \\ exp \\ \\bigg(-\\frac{ (y^{(i)}- \\theta^Tx^{(i)})^2}{2\\sigma^2}\\bigg)$$ The notation $p(y^i|x^i;\\theta)$ indicates that this is the distribution of  $y^{(i)}$ given $x^{(i)}$ and parameterized by $\\theta$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The probability of the data is given by $p( \\vec{y} |X; \\theta )$. This quantity is typically viewed a function of $\\vec{y}$ , for a fixed value of $ \\theta \\ $. When we wish to explicitly view this as a function of $ \\theta$ ,we will instead call it the likelihood function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$ L(\\theta)=L(\\theta , X , \\vec{y} = p(\\vec{y | X ;\\theta)} $$  \n",
    "$$ = \\prod _{i=1} ^{m} p(y^i|X^i; \\theta)  \\\\ = \\prod _{i=1} ^{m} \\frac{1}{\\sqrt{2\\pi }\\sigma} \\ \\ exp \\ \\bigg(-\\frac{ (y^{(i)}- \\theta^Tx^{(i)})^2}{2\\sigma^2}\\bigg)$$ Instead of maximizing $L(\\theta)$ , we can also maximize any strictly increasing function of $L(\\theta)$ .In particular , the derivatives will be a bit simpler if we instead maximize the **log likelyhood**.   $$ l(\\theta ) = \\log L(\\theta) = log \\prod _{i=1} ^{m}  \\frac{1}{\\sqrt{2\\pi }\\sigma} \\ \\ exp \\ \\bigg(-\\frac{ (y^{(i)}- \\theta^Tx^{(i)})^2}{2\\sigma^2}\\bigg) $$  $$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\  =\\Sigma _{i=1} ^{m} log \\frac{1}{\\sqrt{2\\pi }\\sigma} \\ \\ exp \\ \\bigg(-\\frac{ (y^{(i)}- \\theta^Tx^{(i)})^2}{2\\sigma^2}\\bigg) $$ $$ m log \\frac{1}{\\sqrt{2\\pi }\\sigma} - \\frac{1}{\\sigma^2} * \\frac{1}{2} \\Sigma _{i=1} ^{m} \\bigg(y^{(i)} -\\theta^T x^{(i)} \\bigg)^2 $$ which we recognize to be $j(\\theta)$ , our         original least-squares cost function.    $$----*****----$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                          ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
